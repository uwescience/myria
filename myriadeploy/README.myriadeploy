before use:

1) in myria source, run gradle jar and copy myriad-0.1.jar here

2) make sure "java -version" shows 7 on any machines in the cluster. 
You can put java 7 in your directory, and let your PATH include it BEFORE the default PATH

3) copy workers.txt.sample to workers.txt and modify it for your needs
the first line is always the master.

4) better make sure you can access any machine in the cluster from your client side, using ssh with no password.
otherwise you will need to enter them a lot of times.
easiest way to make it happen:
ssh-keygen
ssh-copy-id username@remote_machine_address
use default settings all the way
then ssh to the remote machine and check.
install ssh-copy-id if you don't have it on your machine.



0) setup the cluster
./setup_cluster.py <description> <path_to_working_dir> <node_list>

Use this script if you want to start a new myria cluster configuration.
This will: create a directory called <description> with all the catalog files, then dispatch them to
corresponding machines in <node_list> under <path_to_working_dir>.
Notice, no ingested relations in previous myria instances will be inherited by this new one.


launch the cluster:
1) if you are lazy:
./launch_cluster <description> <path_to_working_dir> <node_list> <max_heap_size(optional)>
e.g.
./launch_cluster.sh fourNodes /tmp/jwang workers.txt -Xmx1g
will pass max_heap_size to jvm if it's specified. 

This script will run the following 2 scripts in order: start_master, start_workers.
You can run this script on any machine where ssh to node_list is possible, like your laptop, hereinafter the same.
Then you are done.

If you want to do it separately:

2) start master
./start_master.py <description> <path_to_working_dir> <node_list> <max_heap_size(optional)>

This will remotely start the master daemon, which includes the REST api server and the "query" server, 
on the machine given in the first line of <node_list>. 
Stderr and stdout are redirected to corresponding files.

3) start workers
./start_workers.py <description> <path_to_working_dir> <node_list> <max_heap_size(optional)>

This will remotely start workers on machines given in <node_list>. 
Stderr and stdout are redirected to corresponding files.



run some queries:

1) query alive workers
curl -i beijing:8753/workers/alive
replace beijing with your server name

2) ingest some data
curl -i -XPOST beijing:8753/dataset -H "Content-type: application/json"  -d @./ingest_twitter.json
examples are in the directory jsonQueries, check them

3) run a query
curl -i -XPOST beijing:8753/query -H "Content-type: application/json"  -d @./global_join.json
examples are in the directory jsonQueries, check them
check the result in master.stdout in your working directory, if the result is sent back to the server

--- above steps are repeatable, until you are willing to let them go ---



shutdown:

1) shutdown the whole cluster
curl -i beijing:8753/server/shutdown
this will shoutdown everything, including the server and all the workers.

2) if anything happened to make the above shutdown not work
./stop_all_by_force <node_list> 
e.g.
./stop_all_by_force workers.txt

this will go to all the nodes, find the master/worker processes under your username, and kill them.

